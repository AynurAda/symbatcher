import concurrent.futures
import logging
import threading
from typing import Any, Callable, Dict, List, Optional, Union
from symai.core_ext import bind
from symai import Expression

lgr = logging.getLogger()
lgr.setLevel(logging.CRITICAL)


class BatchScheduler(Expression):
    """
    A class for scheduling and executing batch operations with Expressions from symbolicai.

    This scheduler manages the concurrent execution of symbolicai Expressions in batches,
    utilizing multiple workers and an external engine for processing.
    """

    def __init__(self):
        """Initialize the BatchScheduler without parameters."""
        super().__init__()
    
    @bind(engine="neurosymbolic", property="__call__")
    def engine(self, *args, **kwargs):
        """
        Engine method that will be bound to the neurosymbolic engine's __call__.
        Must accept arguments to match the engine's interface.
        """
        pass

    def single_expression(self, data_point: Any, **kwargs) -> Any:
        """
        Execute the symbolicai Expression for a single data point.

        Args:
            data_point (Any): The data point to process through the Expression.

        Returns:
            Any: The result of the Expression execution.
        """
        expr = self.expr
        try:
            return expr(data_point, executor_callback=self.executor_callback, **kwargs)
        except Exception as e:
            print(f"Data point {data_point} generated an exception: {str(e)}")
            return e   
 
    def executor_callback(self, argument: Any) -> Any:
        """
        Callback function for the executor to handle arguments and responses from the Expression.

        This method is called by the symbolicai Expression during execution.

        Args:
            argument (Any): The argument generated by the Expression to be processed.

        Returns:
            Any: The processed response for the given argument.
        """
        with self.lock:
            self.arguments.append(argument)
            arg_id = id(argument)
            if arg_id not in self.llm_responses.keys():
                self.llm_responses[arg_id] = None
                self.llm_response_ready[arg_id] = threading.Event()
            if len(self.arguments) >= self.batch_size or self.pending_tasks < self.batch_size:
                self.batch_ready.set()
        self.llm_response_ready[arg_id].wait()
        with self.lock:
            llm_response = self.llm_responses.pop(arg_id)
            del self.llm_response_ready[arg_id]
        return llm_response   
 
    def execute_queries(self) -> None:
        """
        Execute queries in batches using the engine.

        This method runs in a separate thread and processes batches of arguments
        generated by the symbolicai Expressions.
        """
        while not self.processing_complete.is_set() or self.arguments:
            self.batch_ready.wait()
            self.batch_ready.clear()
            with self.lock:
                current_arguments = self.arguments[:self.batch_size]
                self.arguments = self.arguments[self.batch_size:]      
            if current_arguments:
                llm_batch_responses = self.engine()(current_arguments)
                llm_batch_responses = [(resp[0] if isinstance(resp[0], list) else [resp[0]], resp[1]) for resp in llm_batch_responses]
                for arg, llm_response in zip(current_arguments, llm_batch_responses):
                    with self.lock:
                        arg_id = id(arg)
                        self.llm_responses[arg_id] = llm_response
                        self.llm_response_ready[arg_id].set()
            if self.arguments:
                self.batch_ready.set()
 
    def forward(self, expr: Expression, num_workers: int, dataset: List[Any], batch_size: int = 5, **kwargs) -> List[Any]:
        """
        Run the batch scheduling process for symbolicai Expressions.

        Args:
            expr (Expression): The symbolicai Expression to be executed.
            num_workers (int): The number of worker threads to use.
            dataset (List[Any]): The list of data points to process through the Expression.
            batch_size (int, optional): The size of each batch. Defaults to 5.

        Returns:
            List[Any]: A list of final results for each data point in the dataset.
        """
        # Initialize all the instance variables that were previously in __init__
        self.num_workers = num_workers
        self.dataset = dataset
        self.results = {}
        self.arguments = []
        self.lock = threading.Lock()
        self.batch_size = min(batch_size, len(dataset) if dataset else 1, num_workers)
        self.batch_ready = threading.Event()
        self.processing_complete = threading.Event()
        self.llm_responses = {}
        self.llm_response_ready = {}
        self.pending_tasks = len(self.dataset)
        self.expr = expr()

        query_thread = threading.Thread(target=self.execute_queries)
        query_thread.start()
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_data = {executor.submit(self.single_expression, data_point, **kwargs): data_point for data_point in self.dataset}
            for future in concurrent.futures.as_completed(future_to_data):
                data_point = future_to_data[future]
                try:
                    final_result = future.result()
                    self.results[data_point] = final_result
                except Exception as exc:
                    print(f'Data point {data_point} generated an exception: {exc}')
                finally:
                    self.pending_tasks -= 1
                    self.batch_ready.set()
        self.processing_complete.set()
        print("processing complete")
        self.batch_ready.set()   
        query_thread.join()
        return [self.results.get(data_point) for data_point in self.dataset]
